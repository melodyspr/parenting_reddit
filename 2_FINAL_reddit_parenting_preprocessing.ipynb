{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do mothers and fathers talk about parenting to different audiences? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load needed modules\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import requests as rq\n",
    "import json\n",
    "import time\n",
    "import sys ## for printing only\n",
    "import tqdm ## This is for a progress bar\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files\n",
    "selected_daddit = pd.read_excel(r'comments_daddit.xlsx', header=None, names=['author', 'body', 'created_utc', 'subreddit'])\n",
    "selected_daddit_parenting = pd.read_excel(r'comments_parenting_dads.xlsx', header=None, names=['author', 'body', 'created_utc', 'subreddit'])\n",
    "parenting_selection_mommit = pd.read_excel('comments_parenting_moms.xlsx')\n",
    "mommit_selection = pd.read_excel('comments_mommit.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of r/Mommit users who published on r/Parenting and their comments\n",
    "print(len(mommit_selection))\n",
    "print(len(parenting_selection_mommit))\n",
    "print(len(set(mommit_selection['author'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of r/Daddit users who published on r/Parenting and their comments\n",
    "print(len(selected_daddit))\n",
    "print(len(selected_daddit_parenting))\n",
    "print(len(set(selected_daddit_parenting['author'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the four files into two data frame\n",
    "motherhood = pd.concat([mommit_selection, parenting_selection_mommit], ignore_index = True)\n",
    "fatherhood = pd.concat([selected_daddit, selected_daddit_parenting], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column gender\n",
    "fatherhood['gender'] = 'father'\n",
    "motherhood['gender'] = 'mother'\n",
    "#merge the two data frames into one with all authors\n",
    "all_parents = pd.concat([fatherhood, motherhood], ignore_index = True)\n",
    "all_parents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are authors who published on bother r/Mommit and r/Daddit\n",
    "common_authors = set(mommit_selection[\"author\"]).intersection(set(selected_daddit[\"author\"]))\n",
    "len(common_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the previous set, create a list of common authors\n",
    "no_common = []\n",
    "for author in set(all_parents[\"author\"]): \n",
    "    if author not in common_authors:\n",
    "        no_common.append(author)\n",
    "len(no_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the length corresponds to the initial number of authors\n",
    "len(no_common) + len(common_authors)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove missing values\n",
    "all_parents.dropna(inplace = True)\n",
    "#Remove common authors\n",
    "all_parents = all_parents.loc[all_parents['author'].isin(no_common)]\n",
    "all_parents = all_parents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from the dataset the removed submissions\n",
    "removed_comment = 0\n",
    "removed_indices = []\n",
    "for i in range(0,len(all_parents)):\n",
    "    if \"your submission has been removed\" in all_parents['body'][i].lower():\n",
    "        removed_comment += 1\n",
    "        removed_indices.append(i)\n",
    "print(removed_comment)\n",
    "print(removed_indices)\n",
    "\n",
    "all_parents.drop(all_parents.index[removed_indices], inplace = True)\n",
    "all_parents = all_parents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the new length of the data frame\n",
    "len(all_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing deleted comments\n",
    "all_parents = all_parents[all_parents[\"author\"] != \"[deleted]\"]\n",
    "all_parents = all_parents[all_parents[\"author\"] != \"AutoModerator\"]\n",
    "all_parents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will keep only the authors without the word \"bot\" in them (it indicates that they are not real authors)\n",
    "bots_parents = set()\n",
    "for author in all_parents['author']: \n",
    "    if \"bot\" in author.lower(): \n",
    "        bots_parents.add(author)\n",
    "print(bots_parents)\n",
    "print(f\"{len(bots_parents)} bots will be removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(all_parents)} comments in parenthood before removing the bots\")\n",
    "new_bots_parents = list(bots_parents)\n",
    "for author in new_bots_parents:\n",
    "    if author == \"Phlebotanist\" or author == \"BotchedUpElia\" or author == \"redbottleofshampoo\":\n",
    "        new_bots_parents.remove(author)\n",
    "#Removing the bots\n",
    "all_parents = all_parents[~all_parents['author'].isin(new_bots_parents)]\n",
    "print(f\"{len(all_parents)} comments left after removing the ones written by bots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the indexes\n",
    "all_parents = all_parents.reset_index(drop=True)\n",
    "#remove missing values\n",
    "all_parents.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this version\n",
    "all_parents.to_pickle('all_parents_nobots.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of all_parents so the first data frame stays untouched\n",
    "data_clean = all_parents.copy()\n",
    "#Remove missing values\n",
    "data_clean.dropna(inplace = True)\n",
    "#Reset the indexes\n",
    "data_clean = data_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first round of text cleaning techniques\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\",'', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the data frame and apply the first round of cleaning to it\n",
    "data_clean['preprocessed'] = data_clean['body'].copy()\n",
    "data_clean['preprocessed'] = data_clean.preprocessed.apply(round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the second round of cleaning\n",
    "data_clean['preprocessed'] = data_clean['preprocessed'].apply(round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the cleaned version of the file\n",
    "data_clean.to_pickle('data_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to tokenize the data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a version of the data set with only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a version of the dataset with only nouns\n",
    "# create a function to pull out nouns from a string of text\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text, lemmatize it and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    lemmatized =[]\n",
    "    for w in tokenized:\n",
    "        lemmatized.append(lemmatizer.lemmatize(w))\n",
    "    all_nouns = [word for (word, pos) in pos_tag(lemmatized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouns = data_clean.copy()\n",
    "data_nouns.body = data_clean.body.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nouns = data_nouns.copy()\n",
    "tokenized_nouns.body = tokenized_nouns.body.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there empty comments\n",
    "empty_comments = 0\n",
    "for comment in tokenized_nouns['body']:\n",
    "    if len(comment) == 0:\n",
    "        empty_comments += 1\n",
    "print(empty_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty comments\n",
    "tokenized_nouns = tokenized_nouns[tokenized_nouns.astype(str)['body'] != \"[]\"]\n",
    "#Reset the indexes\n",
    "tokenized_nouns = tokenized_nouns.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words\n",
    "#add_stop_words = [\"i\", \"kid\", \"thing\", \"im\", \"wa\", \"youre\", \"lot\", \"dont\", \"thats\", \"shes\", \"ha\", \"anything\", \"everything\", \"bit\", \"part\", \"everyone\", \"one\", \"doesnt\", \"theyre\", \"etc\", \"u\", \"didnt\", \"mine\", \"anyone\", \"isnt\", \"well\", \"yeah\", \"get\", \"yes\", \"while\", \"whats\", \"amount\", \"youve\", \"youll\", \"haha\", \"cant\", \"le\", \"lo\", \"kiddo\", \"arent\", \"ive\", \"wouldnt\", \"op\", \"top\", \"half\", \"let\", \"wont\", \"set\", \"wasnt\", \"none\", \"yours\", \"weve\", \"ask\", \"couldnt\", \"theyll\", \"yo\", \"reddit\", \"ours\", \"go\", \"gon\", \"gt\", \"wish\", \"app\", \"tell\", \"come\", \"want\", \"itll\", \"ok\", \"yep\", \"bc\", \"youd\", \"theyve\", \"okay\", \"nope\", \"thread\", \"oh\", \"aspect\", \"kiddos\", \"omg\", \"shouldnt\", \"take\", \"yr\", \"v\", \"till\", \"push\", \"fine\", \"x\", \"d\", \"mo\", \"hi\", \"b\", \"hers\", \"theyd\", \"yup\", \"hahaha\", \"er\", \"boy\", \"baby\", \"child\", \"parent\", \"way\", \"girl\", \"son\", \"daughter\", \"mom\", \"dad\", \"husband\", \"woman\", \"lol\", \"husband\", \"wife\", \"brother\", \"sister\", \"mother\", \"father\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "#remove stopwords from tokenized dataset\n",
    "tokenized_nouns['body'] = tokenized_nouns['body'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nouns.to_pickle('tokenized_nouns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format), the dtm, and the CountVectorizer object\n",
    "data_nouns.to_pickle('data_nouns.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a version of the data set with nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a version of the dataset with nouns and verbs\n",
    "# create a function to pull out nouns from a string of text\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nouns_verbs(text):\n",
    "    '''Given a string of text, tokenize the text, lemmatize it and pull out only the nouns.'''\n",
    "    is_noun_verb = lambda pos: pos[:2] == 'NN'or pos[:2] == 'VB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    lemmatized =[]\n",
    "    for w in tokenized:\n",
    "        lemmatized.append(lemmatizer.lemmatize(w))\n",
    "    all_nouns = [word for (word, pos) in pos_tag(lemmatized) if is_noun_verb(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouns_verbs = data_clean.copy()\n",
    "data_nouns_verbs.body = data_nouns_verbs.body.apply(nouns_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nouns_verbs = data_nouns_verbs.copy()\n",
    "tokenized_nouns_verbs.body = tokenized_nouns_verbs.body.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there empty comments\n",
    "empty_comments = 0\n",
    "for comment in tokenized_nouns_verbs['body']:\n",
    "    if len(comment) == 0:\n",
    "        empty_comments += 1\n",
    "print(empty_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty comments\n",
    "tokenized_nouns_verbs = tokenized_nouns_verbs[tokenized_nouns_verbs.astype(str)['body'] != \"[]\"]\n",
    "#Reset the indexes\n",
    "tokenized_nouns_verbs = tokenized_nouns_verbs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words\n",
    "#add_stop_words = [\"i\", \"kid\", \"thing\", \"im\", \"wa\", \"youre\", \"lot\", \"dont\", \"thats\", \"shes\", \"ha\", \"anything\", \"everything\", \"bit\", \"part\", \"everyone\", \"one\", \"doesnt\", \"theyre\", \"etc\", \"u\", \"didnt\", \"mine\", \"anyone\", \"isnt\", \"well\", \"yeah\", \"get\", \"yes\", \"while\", \"whats\", \"amount\", \"youve\", \"youll\", \"haha\", \"cant\", \"le\", \"lo\", \"kiddo\", \"arent\", \"ive\", \"wouldnt\", \"op\", \"top\", \"half\", \"let\", \"wont\", \"set\", \"wasnt\", \"none\", \"yours\", \"weve\", \"ask\", \"couldnt\", \"theyll\", \"yo\", \"reddit\", \"ours\", \"go\", \"gon\", \"gt\", \"wish\", \"app\", \"tell\", \"come\", \"want\", \"itll\", \"ok\", \"yep\", \"bc\", \"youd\", \"theyve\", \"okay\", \"nope\", \"thread\", \"oh\", \"aspect\", \"kiddos\", \"omg\", \"shouldnt\", \"take\", \"yr\", \"v\", \"till\", \"push\", \"fine\", \"x\", \"d\", \"mo\", \"hi\", \"b\", \"hers\", \"theyd\", \"yup\", \"hahaha\", \"er\", \"boy\", \"baby\", \"child\", \"parent\", \"way\", \"girl\", \"son\", \"daughter\", \"mom\", \"dad\", \"husband\", \"woman\", \"lol\", \"husband\", \"wife\", \"brother\", \"sister\", \"mother\", \"father\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "#remove stopwords from tokenized dataset\n",
    "tokenized_nouns['body'] = tokenized_nouns['body'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nouns_verbs.to_pickle('tokenized_nouns_verbs.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "reddit-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
