{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do mothers and fathers talk about parenting to different audiences? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load needed modules\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import requests as rq\n",
    "import json\n",
    "import time\n",
    "import sys ## for printing only\n",
    "import tqdm ## This is for a progress bar\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collecting Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date, format='human'):\n",
    "    \"\"\"\"\n",
    "    It takes a string and converts it into either human readable date format or epoch date format\n",
    "    \n",
    "    Parameteres:\n",
    "    ============\n",
    "    date: str\n",
    "        A string with either epoch date format or human readable date format.\n",
    "    format: str\n",
    "        A string defining the format of the input string. By default, it takes the value 'human' and the other option is 'epoch'.\n",
    "    \"\"\"\n",
    "    if format == 'human':\n",
    "        pattern = '%Y-%m-%d %H:%M:%S'\n",
    "        return str(int(time.mktime(time.strptime(date, pattern))))\n",
    "    elif format == 'epoch':\n",
    "        pattern = '%Y-%m-%d %H:%M:%S'\n",
    "        return time.strftime(pattern, time.localtime(int(date)))\n",
    "    \n",
    "def collect_data(source_url, payload):\n",
    "    \"\"\"\n",
    "    It takes the Pushift endpoint and payload as arguments and sends a request to the given URL. Depending on the status code it either returns \n",
    "    the list of mappings, a status code, or sleeps for 60 seconds and tries again to scrape the data.\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    source_url: str\n",
    "        A string with url of the Pushshift endpoint.\n",
    "    payload: \n",
    "        A mapping with parameters passed to the Pushshift API.\n",
    "    \"\"\"\n",
    "    if 'after' not in payload:\n",
    "        payload['after'] = parse_date('2005-06-23 00:00:00')\n",
    "    response = rq.get(source_url, params = payload)\n",
    "    if response.status_code == 200:\n",
    "        time.sleep(1)\n",
    "        return json.loads(response.text)['data']\n",
    "    elif response.status_code == 429 or response.status_code == 523 or response.status_code == 502:\n",
    "        for i in range(60,0,-1):\n",
    "            print(f'\\rThe compulsory break finishes in {str(i)} seconds', end ='', flush=True)\n",
    "            time.sleep(1)\n",
    "        print('\\r' + 100*' ')\n",
    "        return collect_data(source_url = source_url, payload = payload)\n",
    "    else:\n",
    "        return [{'status' : response.status_code, 'message' : response.content }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a file in write mode\n",
    "with open('daddit_comments_2020.jl', 'w') as file:\n",
    "    ## Write out the data you already collected\n",
    "    source_url = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "    payload = { 'subreddit' : 'daddit',\n",
    "                'after' : parse_date('2020-01-01 00:00:00'),\n",
    "                'before' : parse_date('2021-01-01 00:00:00'),\n",
    "                'fields' : [\"author\", \"created_utc\", \"subreddit\", \"body\"],\n",
    "                'sort_type' : \"created_utc\",\n",
    "                'size' : 500}\n",
    "    daddit_comments = collect_data(source_url = source_url, payload = payload)\n",
    "    for line in daddit_comments:\n",
    "        line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "    ## Set the progress bar\n",
    "    pbar = tqdm.tqdm(position=0, leave=True,initial=100)\n",
    "    ## Create a while-loop\n",
    "    while len(daddit_comments) > 0:\n",
    "        ## Check if we got data from Reddit or a strange status code\n",
    "        if len(daddit_comments[0].keys()) > 2:\n",
    "            ## Get the last collected data date in epoch time\n",
    "            after = parse_date(daddit_comments[-1]['created_utc'])\n",
    "            ## Update the payload after field\n",
    "            payload['after'] = after\n",
    "            ## Collect the data\n",
    "            daddit_comments = collect_data(source_url = source_url, payload = payload)\n",
    "            ## Write out the collected data to the file\n",
    "            for line in daddit_comments:\n",
    "                if 'created_utc' in line:\n",
    "                    line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "                    file.write(json.dumps(line) + '\\n')\n",
    "            ## Update the progress bar\n",
    "            pbar.update(len(daddit_comments))\n",
    "        else:\n",
    "            ## Print out the strange status code and its message\n",
    "            print(f'Something went wrong. The status code error was {daddit_comments.pop}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the file with collected data\n",
    "daddit_comments_2020 = pd.read_json(r'daddit_comments_2020.jl', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the set of unique authors, \n",
    "#make it a list (to fix the order if there is a bug and we want to collect the rest of authors)\n",
    "#sort them in alphabetical order\n",
    "authors_daddit_2020 = sorted(list(set(daddit_comments_2020[\"author\"])), key=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect the comments published by each author who posted on r/Daddit in 2020\n",
    "## Open a file in write mode\n",
    "with open('authors_daddit_comments_2020.jl', 'a') as file:\n",
    "    ## Set the progress bar\n",
    "    pbar = tqdm.tqdm(position=0, leave=True,initial=100)\n",
    "    #take each author of the list and collect their comments published on r/Parenting subreddit in 2020\n",
    "    for author in authors_daddit_2020:\n",
    "        source_url = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "        payload = { 'after' : parse_date('2020-01-01 00:00:00'),\n",
    "                    'before' : parse_date('2020-12-31 00:00:00'),\n",
    "                    'subreddit' : \"Parenting\"\n",
    "                    'author' : author,\n",
    "                    'fields' : [\"author\", \"created_utc\", \"subreddit\", \"body\"],\n",
    "                    'sort_type' : \"created_utc\",\n",
    "                    'size' : 100}\n",
    "        daddit_comments = collect_data(source_url = source_url, payload = payload)\n",
    "        for line in daddit_comments:\n",
    "            line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "            file.write(json.dumps(line) + '\\n')\n",
    "        ## Create a while-loop\n",
    "        while len(daddit_comments) > 0:\n",
    "            ## Check if we got data from Reddit or a strange status code\n",
    "            if len(daddit_comments[0].keys()) > 2:\n",
    "                ## Get the last collected data date in epoch time\n",
    "                after = parse_date(daddit_comments[-1]['created_utc'])\n",
    "                ## Update the payload after field\n",
    "                payload['after'] = after\n",
    "                ## Collect the data\n",
    "                daddit_comments = collect_data(source_url = source_url, payload = payload)\n",
    "                ## Write out the collected data to the file\n",
    "                for line in daddit_comments:\n",
    "                    if 'created_utc' in line:\n",
    "                        line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "                        file.write(json.dumps(line) + '\\n')\n",
    "            else:\n",
    "                ## Print out the strange status code and its message\n",
    "                print(f'Something went wrong. The status code error was {daddit_comments.pop}.')\n",
    "        ## Update the progress bar\n",
    "        pbar.update(len(daddit_comments))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "reddit-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
